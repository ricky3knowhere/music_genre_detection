{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ricky\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "import librosa\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import soundfile as sf\n",
    "from pedalboard.io import AudioFile\n",
    "from pedalboard import *\n",
    "import noisereduce as nr\n",
    "from tensorflow.image import resize\n",
    "import sounddevice as sd\n",
    "import time\n",
    "import scipy.io.wavfile as wav\n",
    "from pedalboard.io import AudioFile\n",
    "from pedalboard import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 Microsoft Sound Mapper - Input, MME (2 in, 0 out)\n",
      ">  1 Microphone (WO Mic Device), MME (2 in, 0 out)\n",
      "   2 Microphone Array (Realtek(R) Au, MME (2 in, 0 out)\n",
      "   3 Headset (Terra 3 Mini), MME (2 in, 0 out)\n",
      "   4 Microsoft Sound Mapper - Output, MME (0 in, 2 out)\n",
      "<  5 Headphones (Terra 3 Mini), MME (0 in, 2 out)\n",
      "   6 Speakers (Realtek(R) Audio), MME (0 in, 2 out)\n",
      "   7 Primary Sound Capture Driver, Windows DirectSound (2 in, 0 out)\n",
      "   8 Microphone (WO Mic Device), Windows DirectSound (2 in, 0 out)\n",
      "   9 Microphone Array (Realtek(R) Audio), Windows DirectSound (2 in, 0 out)\n",
      "  10 Headset (Terra 3 Mini), Windows DirectSound (2 in, 0 out)\n",
      "  11 Primary Sound Driver, Windows DirectSound (0 in, 2 out)\n",
      "  12 Headphones (Terra 3 Mini), Windows DirectSound (0 in, 2 out)\n",
      "  13 Speakers (Realtek(R) Audio), Windows DirectSound (0 in, 2 out)\n",
      "  14 Speakers (Realtek(R) Audio), Windows WASAPI (0 in, 2 out)\n",
      "  15 Headphones (Terra 3 Mini), Windows WASAPI (0 in, 2 out)\n",
      "  16 Microphone Array (Realtek(R) Audio), Windows WASAPI (2 in, 0 out)\n",
      "  17 Microphone (WO Mic Device), Windows WASAPI (1 in, 0 out)\n",
      "  18 Headset (Terra 3 Mini), Windows WASAPI (1 in, 0 out)\n",
      "  19 Speakers 1 (Realtek HD Audio output with SST), Windows WDM-KS (0 in, 2 out)\n",
      "  20 Speakers 2 (Realtek HD Audio output with SST), Windows WDM-KS (0 in, 8 out)\n",
      "  21 PC Speaker (Realtek HD Audio output with SST), Windows WDM-KS (2 in, 0 out)\n",
      "  22 Microphone Array (Realtek HD Audio Mic input), Windows WDM-KS (2 in, 0 out)\n",
      "  23 Stereo Mix (Realtek HD Audio Stereo input), Windows WDM-KS (2 in, 0 out)\n",
      "  24 Input (), Windows WDM-KS (2 in, 0 out)\n",
      "  25 Headphones (), Windows WDM-KS (0 in, 2 out)\n",
      "  26 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(AIRBOOM NITRO X)), Windows WDM-KS (0 in, 1 out)\n",
      "  27 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(AIRBOOM NITRO X)), Windows WDM-KS (1 in, 0 out)\n",
      "  28 Headphones (), Windows WDM-KS (0 in, 2 out)\n",
      "  29 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Bluetooth  Music)), Windows WDM-KS (0 in, 1 out)\n",
      "  30 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Bluetooth  Music)), Windows WDM-KS (1 in, 0 out)\n",
      "  31 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Terra 3 Mini)), Windows WDM-KS (0 in, 1 out)\n",
      "  32 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Terra 3 Mini)), Windows WDM-KS (1 in, 0 out)\n",
      "  33 Microphone (WO Mic Wave), Windows WDM-KS (1 in, 0 out)\n",
      "  34 Headphones (), Windows WDM-KS (0 in, 2 out)\n",
      "  35 Headphones (), Windows WDM-KS (0 in, 2 out)\n",
      "  36 Headphones (), Windows WDM-KS (0 in, 2 out)\n",
      "  37 Output (@System32\\drivers\\bthhfenum.sys,#4;%1 Hands-Free HF Audio%0\n",
      ";(Galaxy J3 Pro)), Windows WDM-KS (0 in, 1 out)\n",
      "  38 Input (@System32\\drivers\\bthhfenum.sys,#4;%1 Hands-Free HF Audio%0\n",
      ";(Galaxy J3 Pro)), Windows WDM-KS (1 in, 0 out)\n",
      "  39 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(CHARGE MINI)), Windows WDM-KS (0 in, 1 out)\n",
      "  40 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(CHARGE MINI)), Windows WDM-KS (1 in, 0 out)\n",
      "  41 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Fit 3)), Windows WDM-KS (0 in, 1 out)\n",
      "  42 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Fit 3)), Windows WDM-KS (1 in, 0 out)\n",
      "  43 Headphones (), Windows WDM-KS (0 in, 2 out)\n",
      "  44 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(208)), Windows WDM-KS (0 in, 1 out)\n",
      "  45 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(208)), Windows WDM-KS (1 in, 0 out)\n"
     ]
    }
   ],
   "source": [
    "print(sd.query_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muscic Record 🎙️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mulai merekam...\n",
      "Rekaman selesai.\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 44100\n",
    "\n",
    "def save_audio(filename, sample_rate, data):\n",
    "    max_val = np.max(np.abs(data))\n",
    "    if max_val > 0:\n",
    "        data = data / max_val\n",
    "\n",
    "    wav.write(filename, sample_rate, (data * 32767).astype(np.int16))\n",
    "\n",
    "\n",
    "# print(\"Recording...\")\n",
    "# recording = np.zeros(0)\n",
    "\n",
    "# def callback(indata, frames, time, status):\n",
    "#     global recording\n",
    "#     recording = np.append(recording, indata[:, 0])\n",
    "\n",
    "# stream = sd.InputStream(\n",
    "#     callback=callback, channels=1, samplerate=sample_rate, blocksize=1024\n",
    "# )\n",
    "# stream.start()\n",
    "\n",
    "# start_time = time.time()\n",
    "# while time.time() - start_time < 30:\n",
    "#     time.sleep(0.5)  # Update every 100 ms\n",
    "    \n",
    "# stream.stop()\n",
    "# stream.close()\n",
    "# print(\"Recording finished.\")\n",
    "\n",
    "print(\"Mulai merekam...\")\n",
    "recording = sd.rec(int(30 * sample_rate), samplerate=sample_rate, channels=1, dtype='float64')\n",
    "sd.wait()  # Tunggu hingga rekaman selesai\n",
    "print(\"Rekaman selesai.\")\n",
    "\n",
    "save_audio(\"scipy.wav\", sample_rate, recording)\n",
    "\n",
    "with AudioFile('scipy.wav').resampled_to(sample_rate) as f:\n",
    "    audio = f.read(f.frames)\n",
    "\n",
    "# audio = nr.reduce_noise(y=audio, sr=sample_rate, stationary=True, prop_decrease=0.5)\n",
    "\n",
    "board = Pedalboard([\n",
    "    NoiseGate(threshold_db=-50, ratio=1.8, release_ms=250),\n",
    "    Compressor(threshold_db=-15, ratio=2.5),\n",
    "    # LowShelfFilter(cutoff_frequency_hz=400, gain_db=10, q=1),\n",
    "    Gain(gain_db=10)\n",
    "])\n",
    "\n",
    "effected = board(audio, sample_rate)\n",
    "\n",
    "\n",
    "with AudioFile('audio2_enhenced.wav', 'w', sample_rate, effected.shape[0]) as f:\n",
    "    f.write(effected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Init ⚙️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_chitapatel = load_model(\"./model/custom_cnn_0.0031.h5\")\n",
    "model_tripathi = load_model(\"./model/tripathi_model.h5\")\n",
    "model_val_v1_2 = load_model(\"./model/cnn__genre_detection_0.93.h5\")\n",
    "model_val_v2_3 = load_model(\"./model/cnn__genre_detection_0.92.h5\")\n",
    "model_val_v2_4 = load_model(\"./model/cnn__genre_detection_44100hz_0.91.h5\")\n",
    "model_val_v2_6 = load_model(\"./model/cnn__genre_detection_41100hz_0.95(Tripathi Dataset).h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Requirements 🧪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE_1 = 22050\n",
    "SAMPLE_RATE_2 = 44100\n",
    "\n",
    "TRACK_DURATION = 30  # second\n",
    "# file_path = \"../mfcc_genre_detection/datasets/jazz/jazz.00037.wav\"\n",
    "# file_path = \"../audio_tester/armada - hargai aku-[AudioTrimmer.com].mp3\"\n",
    "# file_path = \"../../../Musics/2022+/SZA - Kill Bill (Audio).mp3\"\n",
    "# file_path = \"../../../Musics/gama - satu ataw dua.mp3\"\n",
    "# file_path = \"../../../Musics/2020s/Ava Max - Salt (Lyrics).mp3\"\n",
    "# file_path = \"../../../Musics/2020s/Qorygore - Sampurasun (Lyrics) ft. Pretty Rico.mp3\"\n",
    "# file_path = \"../../../Musics/2020s/Dj Quads - Punch.mp3\"\n",
    "# file_path = \"../../../Musics/2020s/mxmtoon - ok on your own (ft. Carly Rae Jepsen)  audio .mp3\"\n",
    "file_path = \"scipy.wav\"\n",
    "file_path_2 = \"scipy_jazz.wav\"\n",
    "file_path_3 = \"audio2_enhenced.wav\"\n",
    "# file_path_4 = \"audio4_enhenced.wav\"\n",
    "\n",
    "genres = {\n",
    "    \"Blues\": 0,\n",
    "    \"Classical\": 0,\n",
    "    \"Country\": 0,\n",
    "    \"Disco\": 0,\n",
    "    \"HipHop\": 0,\n",
    "    \"Jazz\": 0,\n",
    "    \"Metal\": 0,\n",
    "    \"Pop\": 0,\n",
    "    \"Reggae\": 0,\n",
    "    \"Rock\": 0,\n",
    "}\n",
    "\n",
    "genres_list = list(genres.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils 🪄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CHITAPATEL MDOEL UTILS\n",
    "def splitsongs(X,window=0.05, overlap=0.5):\n",
    "    # Empty lists to hold our results\n",
    "    temp_X = []\n",
    "\n",
    "    # Get the input song array size\n",
    "    xshape = X.shape[0]\n",
    "    chunk = int(xshape * window)\n",
    "    offset = int(chunk * (1.0 - overlap))\n",
    "\n",
    "    # Split the song and create new ones on windows\n",
    "    spsong = [X[i : i + chunk] for i in range(0, xshape - chunk + offset, offset)]\n",
    "    for s in spsong:\n",
    "        if s.shape[0] != chunk:\n",
    "            continue\n",
    "\n",
    "        temp_X.append(s)\n",
    "\n",
    "    return np.array(temp_X)\n",
    "\n",
    "def to_melspectrogram(songs, n_fft=1024, hop_length=256):\n",
    "    # Transformation function\n",
    "    melspec = lambda x: librosa.feature.melspectrogram(\n",
    "        y=x, sr=22050, n_fft=n_fft, hop_length=hop_length, n_mels=128\n",
    "    )[\n",
    "        :, :, np.newaxis\n",
    "    ]  # keep n_mels=128. other values are for experimenting\n",
    "\n",
    "    # map transformation of input songs to melspectrogram using log-scale\n",
    "    tsongs = map(melspec, songs)\n",
    "    # np.array([librosa.power_to_db(s, ref=np.max) for s in list(tsongs)])\n",
    "    return np.array(list(tsongs))\n",
    "\n",
    "\n",
    "def chitapatel_service(model, file_path):\n",
    "  sample_rate = 22000\n",
    "  signal, sr = librosa.load(file_path, sr=sample_rate)\n",
    "\n",
    "  # start_signal = (sample_rate * 60)\n",
    "  # end_signal = (sample_rate * 90)\n",
    "  # signal = signal[start_signal:end_signal]\n",
    "  signal = signal[:sample_rate * 30]\n",
    "\n",
    "  # signal = nr.reduce_noise(y=signal, sr=sample_rate)\n",
    "  print(signal.shape)\n",
    "\n",
    "  signals = splitsongs(signal, window=0.05)\n",
    "  x = to_melspectrogram(signals)\n",
    "  # x = np.array(x[0])\n",
    "  # x = x[..., np.newaxis]\n",
    "  # x = x[np.newaxis,...]\n",
    "  print(x.shape)\n",
    "  # x = x.reshape(1, 128, 129, 1)\n",
    "  # print(x)\n",
    "  prediction = model.predict(x)\n",
    "\n",
    "  return prediction\n",
    "\n",
    "\n",
    "# VALERIO MODEL UTILS\n",
    "def extract_mfcc(file_path, sr,num_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):\n",
    "    data = []\n",
    "    SAMPLES_PER_TRACK = sr  * TRACK_DURATION\n",
    "    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
    "    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
    "\n",
    "    signal, sample_rate = librosa.load(file_path, sr=sr)\n",
    "    # signal = signal[(SAMPLE_RATE * 40):(SAMPLE_RATE * 70)]\n",
    "    # signal = nr.reduce_noise(y=signal, sr=SAMPLE_RATE)\n",
    "    for d in range(num_segments):\n",
    "        # calculate start and finish sample for current segment\n",
    "        start = samples_per_segment * d\n",
    "        finish = start + samples_per_segment\n",
    "        # extract mfcc\n",
    "        mfcc = librosa.feature.mfcc(\n",
    "            y=signal[start:finish],\n",
    "            sr=sample_rate,\n",
    "            n_mfcc=num_mfcc,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "        )\n",
    "\n",
    "        mfcc = mfcc.T\n",
    "        # store only mfcc feature with expected number of vectors\n",
    "        if len(mfcc) == num_mfcc_vectors_per_segment:\n",
    "            data.append(mfcc.tolist())\n",
    "\n",
    "    return data\n",
    "\n",
    "def valerio_service(model,file_path, sr):\n",
    "    val = extract_mfcc(file_path=file_path, num_segments=10, sr=sr)\n",
    "    val = np.array(val)\n",
    "    print(val.shape)\n",
    "    val = val[..., np.newaxis]\n",
    "    return model.predict(val)\n",
    "\n",
    "\n",
    "# TRIPATHI MODEL UTILS\n",
    "def load_and_preprocess_data(file_path, target_shape=(150, 150)):\n",
    "    data = []\n",
    "    audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "    # Perform preprocessing (e.g., convert to Mel spectrogram and resize)\n",
    "    # Define the duration of each chunk and overlap\n",
    "    chunk_duration = 4  # seconds\n",
    "    overlap_duration = 2  # seconds\n",
    "    print(sample_rate)\n",
    "                \n",
    "    # Convert durations to samples\n",
    "    chunk_samples = chunk_duration * sample_rate\n",
    "    overlap_samples = overlap_duration * sample_rate\n",
    "                \n",
    "    # Calculate the number of chunks\n",
    "    num_chunks = int(np.ceil((len(audio_data) - chunk_samples) / (chunk_samples - overlap_samples))) + 1\n",
    "                \n",
    "    # Iterate over each chunk\n",
    "    for i in range(num_chunks):\n",
    "                    # Calculate start and end indices of the chunk\n",
    "        start = i * (chunk_samples - overlap_samples)\n",
    "        end = start + chunk_samples\n",
    "                    \n",
    "                    # Extract the chunk of audio\n",
    "        chunk = audio_data[start:end]\n",
    "                    \n",
    "                    # Compute the Mel spectrogram for the chunk\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=chunk, sr=sample_rate)\n",
    "                    \n",
    "                #mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
    "        mel_spectrogram = resize(np.expand_dims(mel_spectrogram, axis=-1), target_shape)\n",
    "        data.append(mel_spectrogram)\n",
    "    \n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "def tripathi_service(model, file_path):\n",
    "    X_test = load_and_preprocess_data(file_path)\n",
    "    # print(X_test.shape)\n",
    "    return  model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compile 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(660000,)\n",
      "(39, 128, 129, 1)\n",
      "2/2 [==============================] - 0s 24ms/step\n",
      "44100\n",
      "1/1 [==============================] - 0s 366ms/step\n",
      "(10, 130, 13)\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "(10, 259, 13)\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "(10, 259, 13)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "44100\n",
      "1/1 [==============================] - 0s 86ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_1 = chitapatel_service(model_chitapatel, file_path)\n",
    "pred_2 = tripathi_service(model_tripathi, file_path)\n",
    "pred_6 = valerio_service(model_val_v1_2, file_path, SAMPLE_RATE_1)\n",
    "pred_8 = valerio_service(model_val_v2_3, file_path, SAMPLE_RATE_2)\n",
    "pred_9 = valerio_service(model_val_v2_4, file_path, SAMPLE_RATE_2)\n",
    "pred_11 =tripathi_service(model_val_v2_6, file_path) \n",
    "\n",
    "# pred_1 = chitapatel_service(model_chitapatel, file_path_2)\n",
    "# pred_2 = tripathi_service(model_tripathi, file_path_2)\n",
    "# pred_6 = valerio_service(model_val_v1_2, file_path_2, SAMPLE_RATE_1)\n",
    "# pred_8 = valerio_service(model_val_v2_3, file_path_2, SAMPLE_RATE_2)\n",
    "# pred_9 = valerio_service(model_val_v2_4, file_path_2, SAMPLE_RATE_2)\n",
    "# pred_11 =tripathi_service(model_val_v2_6, file_path_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation 📐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======//==============//==============//=======\n",
      "\t\tCHITAPATEL MODEL\n",
      "=======//==============//==============//=======\n",
      "\n",
      "\n",
      "<<========= Genre Detection Accuracy =======>>\n",
      "Blues\t\t==> 20.0%\n",
      "Classical\t\t==> 0.02%\n",
      "Country\t\t==> 8.73%\n",
      "Disco\t\t==> 9.57%\n",
      "HipHop\t\t==> 6.22%\n",
      "Jazz\t\t==> 0.09%\n",
      "Metal\t\t==> 18.73%\n",
      "Pop\t\t==> 1.36%\n",
      "Reggae\t\t==> 7.86%\n",
      "Rock\t\t==> 27.43%\n",
      "total_val ==> 1.0000000139658027\n",
      "\n",
      "\n",
      "=======//==============//==============//=======\n",
      "\t\tTRIPATHI MODEL\n",
      "=======//==============//==============//=======\n",
      "\n",
      "\n",
      "<<========= Genre Detection Accuracy =======>>\n",
      "Blues\t\t==> 53.32%\n",
      "Classical\t\t==> 0.01%\n",
      "Country\t\t==> 13.69%\n",
      "Disco\t\t==> 5.68%\n",
      "HipHop\t\t==> 19.62%\n",
      "Jazz\t\t==> 0.01%\n",
      "Metal\t\t==> 5.34%\n",
      "Pop\t\t==> 0.29%\n",
      "Reggae\t\t==> 0.25%\n",
      "Rock\t\t==> 1.79%\n",
      "total_val ==> 1.0000000097090715\n",
      "\n",
      "=======//==============//==============//=======\n",
      "\t\tVALERIO MODEL (22050Hz)\n",
      "=======//==============//==============//=======\n",
      "\n",
      "\n",
      "<<========= Genre Detection Accuracy =======>>\n",
      "Blues\t\t==> 21.94%\n",
      "Classical\t\t==> 0.5%\n",
      "Country\t\t==> 0.84%\n",
      "Disco\t\t==> 0.39%\n",
      "HipHop\t\t==> 1.47%\n",
      "Jazz\t\t==> 0.42%\n",
      "Metal\t\t==> 2.99%\n",
      "Pop\t\t==> 34.25%\n",
      "Reggae\t\t==> 34.96%\n",
      "Rock\t\t==> 2.24%\n",
      "total_val ==> 1.0000000004243277\n",
      "\n",
      "=======//==============//==============//=======\n",
      "\t\tVALERIO MODEL (44100Hz)\n",
      "=======//==============//==============//=======\n",
      "\n",
      "\n",
      "<<========= Genre Detection Accuracy =======>>\n",
      "Blues\t\t==> 1.5%\n",
      "Classical\t\t==> 50.44%\n",
      "Country\t\t==> 0.11%\n",
      "Disco\t\t==> 0.49%\n",
      "HipHop\t\t==> 13.87%\n",
      "Jazz\t\t==> 0.95%\n",
      "Metal\t\t==> 7.23%\n",
      "Pop\t\t==> 5.2%\n",
      "Reggae\t\t==> 10.14%\n",
      "Rock\t\t==> 10.1%\n",
      "total_val ==> 0.9999999991888873\n",
      "\n",
      "\n",
      "<<========= Genre Detection Accuracy =======>>\n",
      "Blues\t\t==> 0.99%\n",
      "Classical\t\t==> 3.38%\n",
      "Country\t\t==> 0.19%\n",
      "Disco\t\t==> 5.01%\n",
      "HipHop\t\t==> 70.2%\n",
      "Jazz\t\t==> 0.2%\n",
      "Metal\t\t==> 1.88%\n",
      "Pop\t\t==> 10.72%\n",
      "Reggae\t\t==> 3.16%\n",
      "Rock\t\t==> 4.28%\n",
      "total_val ==> 0.9999999808793542\n",
      "\n",
      "\n",
      "<<========= Genre Detection Accuracy =======>>\n",
      "Blues\t\t==> 27.55%\n",
      "Classical\t\t==> 0.24%\n",
      "Country\t\t==> 2.43%\n",
      "Disco\t\t==> 5.0%\n",
      "HipHop\t\t==> 61.46%\n",
      "Jazz\t\t==> 0.89%\n",
      "Metal\t\t==> 0.27%\n",
      "Pop\t\t==> 0.3%\n",
      "Reggae\t\t==> 1.55%\n",
      "Rock\t\t==> 0.32%\n",
      "total_val ==> 0.9999999974350128\n"
     ]
    }
   ],
   "source": [
    "def model_evaluation(prediction):\n",
    "  # print(prediction.shape[0])\n",
    "  for i in range(prediction.shape[0]):\n",
    "    #     genres[current_genre] = value\n",
    "\n",
    "    for key, v in enumerate(prediction[i]):\n",
    "\n",
    "        # genres[genres_list[key]] = genres[genres_list[key]] + v\n",
    "        genres[genres_list[key]] = ((genres[genres_list[key]] * 3)+ v) / 4\n",
    "\n",
    "  print(\"\\n\\n<<========= Genre Detection Accuracy =======>>\")\n",
    "  # print(genres)\n",
    "  total_val = 0\n",
    "  for genre in genres:\n",
    "      print(\"{}\\t\\t==> {}%\".format(genre, round(genres[genre] * 100, 2)))\n",
    "      total_val = total_val + genres[genre]\n",
    "\n",
    "  print(\"total_val ==>\", total_val)\n",
    "\n",
    "def model_evaluation_valerio(prediction):\n",
    "  # print(prediction.shape[0])\n",
    "  for i in range(prediction.shape[0]):\n",
    "    #     genres[current_genre] = value\n",
    "\n",
    "    for key, v in enumerate(prediction[i]):\n",
    "\n",
    "        # genres[genres_list[key]] = genres[genres_list[key]] + v\n",
    "        genres[genres_list[key]] = ((genres[genres_list[key]] * 3)+ v) / 4\n",
    "\n",
    "  print(\"\\n\\n<<========= Genre Detection Accuracy =======>>\")\n",
    "  # print(genres)\n",
    "  total_val = 0\n",
    "  for genre in genres:\n",
    "      print(\"{}\\t\\t==> {}%\".format(genre, round(genres[genre] * 100, 2)))\n",
    "      total_val = total_val + genres[genre]\n",
    "\n",
    "  print(\"total_val ==>\", total_val)\n",
    "\n",
    "\n",
    "print('\\n'+'=======//======='*3)\n",
    "print('\\t\\tCHITAPATEL MODEL')\n",
    "print('=======//======='*3)\n",
    "model_evaluation(pred_1)\n",
    "\n",
    "print('\\n\\n'+'=======//======='*3)\n",
    "print('\\t\\tTRIPATHI MODEL')\n",
    "print('=======//======='*3)\n",
    "model_evaluation(pred_2)\n",
    "\n",
    "print('\\n'+'=======//======='*3)\n",
    "print('\\t\\tVALERIO MODEL (22050Hz)')\n",
    "print('=======//======='*3)\n",
    "model_evaluation_valerio(pred_6)\n",
    "\n",
    "print('\\n'+'=======//======='*3)\n",
    "print('\\t\\tVALERIO MODEL (44100Hz)')\n",
    "print('=======//======='*3)\n",
    "model_evaluation_valerio(pred_8)\n",
    "model_evaluation_valerio(pred_9)\n",
    "model_evaluation_valerio(pred_11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏬"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
