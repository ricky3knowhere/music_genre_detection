{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce267ebc-0d1d-49d5-9f90-26510ce3107f",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb062bfb-053b-4c4e-b74d-3b940901aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.image import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa0319f-b720-4374-9270-61a5de33df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Model\n",
    "model = tf.keras.models.load_model(\"model/tripathi_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a189f2-8285-4597-8273-ca7653bd8aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['blues', 'classical','country','disco','hiphop','jazz','metal','pop','reggae','rock']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ab4fb-24f4-4754-8d12-9d95695e80a0",
   "metadata": {},
   "source": [
    "### Single Audio Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "842e9eb7-a2d9-4af4-8817-bc3abc85490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess audio data\n",
    "def load_and_preprocess_data(file_path, target_shape=(150, 150)):\n",
    "    data = []\n",
    "    audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "    # Perform preprocessing (e.g., convert to Mel spectrogram and resize)\n",
    "    # Define the duration of each chunk and overlap\n",
    "    chunk_duration = 4  # seconds\n",
    "    overlap_duration = 2  # seconds\n",
    "    print(sample_rate)\n",
    "                \n",
    "    # Convert durations to samples\n",
    "    chunk_samples = chunk_duration * sample_rate\n",
    "    overlap_samples = overlap_duration * sample_rate\n",
    "                \n",
    "    # Calculate the number of chunks\n",
    "    num_chunks = int(np.ceil((len(audio_data) - chunk_samples) / (chunk_samples - overlap_samples))) + 1\n",
    "                \n",
    "    # Iterate over each chunk\n",
    "    for i in range(num_chunks):\n",
    "                    # Calculate start and end indices of the chunk\n",
    "        start = i * (chunk_samples - overlap_samples)\n",
    "        end = start + chunk_samples\n",
    "                    \n",
    "                    # Extract the chunk of audio\n",
    "        chunk = audio_data[start:end]\n",
    "                    \n",
    "                    # Compute the Mel spectrogram for the chunk\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=chunk, sr=sample_rate)\n",
    "                    \n",
    "                #mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
    "        mel_spectrogram = resize(np.expand_dims(mel_spectrogram, axis=-1), target_shape)\n",
    "        data.append(mel_spectrogram)\n",
    "    \n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cfe79f-cbd1-4f54-9f79-3e17256a55a4",
   "metadata": {},
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bc312e2-19bd-44cb-8ae1-baaead4846a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22050\n",
      "1/1 [==============================] - 0s 410ms/step\n",
      "{'Blues': 2.455440859241325e-07, 'Classical': 7.19126872859535e-10, 'Country': 2.6487469101625256e-06, 'Disco': 0.00041205812478795534, 'HipHop': 0.44067096039137976, 'Jazz': 1.7965058486871723e-07, 'Metal': 1.0291056324755685e-05, 'Pop': 0.5366565873524766, 'Reggae': 0.0024198757358396693, 'Rock': 0.002009231722174808}\n",
      "<<========= Genre Detection Accuracy =======>>\n",
      "Blues\t\t==> 0.0%\n",
      "Classical\t\t==> 0.0%\n",
      "Country\t\t==> 0.0%\n",
      "Disco\t\t==> 0.04%\n",
      "HipHop\t\t==> 44.07%\n",
      "Jazz\t\t==> 0.0%\n",
      "Metal\t\t==> 0.0%\n",
      "Pop\t\t==> 53.67%\n",
      "Reggae\t\t==> 0.24%\n",
      "Rock\t\t==> 0.2%\n",
      "total value 98.22\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Audio\n",
    "# file_path = \"../mfcc_genre_detection/datasets/reggae/reggae.00033.wav\"\n",
    "# file_path = \"audio1_enhenced.wav\"\n",
    "file_path = \"test.wav\"\n",
    "# y, sr = librosa.load(file_path, sr=44100)\n",
    "# Audio(data=y, rate=sr)\n",
    "#Processing Test File\n",
    "X_test = load_and_preprocess_data(file_path)\n",
    "# print(X_test.shape)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "genres = {\n",
    "    \"Blues\": 0,\n",
    "    \"Classical\": 0,\n",
    "    \"Country\": 0,\n",
    "    \"Disco\": 0,\n",
    "    \"HipHop\": 0,\n",
    "    \"Jazz\": 0,\n",
    "    \"Metal\": 0,\n",
    "    \"Pop\": 0,\n",
    "    \"Reggae\": 0,\n",
    "    \"Rock\": 0,\n",
    "}\n",
    "genres_list = list(genres.keys())\n",
    "\n",
    "for i in range(prediction.shape[0]):\n",
    "    # print(np.argmax(prediction[i]))\n",
    "    for idx, v in enumerate(prediction[i]):\n",
    "        # print('val', genres['Reggae'])\n",
    "        genres[genres_list[idx]] = ((genres[genres_list[idx]] * 3 )+ v) / 4\n",
    "        # genres[genres_list[idx]] = genres[genres_list[idx]]+ v \n",
    "\n",
    "\n",
    "print(genres)\n",
    "\n",
    "print(\"<<========= Genre Detection Accuracy =======>>\")\n",
    "total_val = 0\n",
    "for genre in genres:\n",
    "    print(\"{}\\t\\t==> {}%\".format(genre, round(genres[genre] * 100, 2)))\n",
    "    total_val = total_val +  round(genres[genre] * 100, 2)\n",
    "\n",
    "print('total value',total_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6e3c708-3604-4c2a-876e-f9dcb63ff361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Prediction\n",
    "def model_prediction(X_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    predicted_categories = np.argmax(y_pred,axis=1)\n",
    "    unique_elements, counts = np.unique(predicted_categories, return_counts=True)\n",
    "    print(y_pred.shape)\n",
    "    max_count = np.max(counts)\n",
    "    print(max_count)\n",
    "    max_elements = unique_elements[counts == max_count]\n",
    "    print(max_elements)\n",
    "    return max_elements[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "661374df-ce68-4ab6-a83a-6561a61ecaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 411ms/step\n",
      "(15, 10)\n",
      "6\n",
      "[1]\n",
      "Model Prediction :: Music Genre --> classical\n"
     ]
    }
   ],
   "source": [
    "#Model Prediction\n",
    "c_index = model_prediction(X_test)\n",
    "print(f\"Model Prediction :: Music Genre --> {classes[c_index]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
